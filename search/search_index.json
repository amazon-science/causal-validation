{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Causal Validation","text":"<p>Causal Validation is a library designed to validate and test your causal models. To achieve this, we provide functionality to simulate causal data, and vaildate your model through a placebo test. </p>"},{"location":"#data-synthesis","title":"Data Synthesis","text":"<p>Data Synthesis in Causal Validation is a fully composable process whereby a set of functions are sequentially applied to a dataset. At some point in this process we also induce a treatment effect. Any of these functions can be parameterised to either have constant parameter values across all control units, or a value that varies across parameters. To see this, consider the below example where we simulate a dataset whose trend varies across each of the 10 control units.</p> <pre><code>from causal_validation import Config, simulate\nfrom causal_validation.effects import StaticEffect\nfrom causal_validation.plotters import plot\nfrom causal_validation.transforms import Trend, Periodic\nfrom causal_validation.transforms.parameter import UnitVaryingParameter\nfrom scipy.stats import norm\n\ncfg = Config(\n    n_control_units=10,\n    n_pre_intervention_timepoints=60,\n    n_post_intervention_timepoints=30,\n)\n\n# Simulate the base observation\nbase_data = simulate(cfg)\n\n# Apply a linear trend with unit-varying intercept\nintercept = UnitVaryingParameter(sampling_dist = norm(0, 1))\ntrend_component = Trend(degree=1, coefficient=0.1, intercept=intercept)\ntrended_data = trend_component(base_data)\n\n# Simulate a 5% lift in the treated unit's post-intervention data\neffect = StaticEffect(0.05)\ninflated_data = effect(trended_data)\n</code></pre> <p></p>"},{"location":"#model-validation","title":"Model Validation","text":"<p>Once a dataset has been synthesised, we may wish to validate our model using a placebo test. In Causal Validation this is straightforward and can be accomplished in combination with AZCausal by the following.</p> <pre><code>from azcausal.estimators.panel.sdid import SDID\nfrom causal_validation.validation.placebo import PlaceboTest\n\nmodel = AZCausalWrapper(model=SDID())\nresult = PlaceboTest(model, inflated_data).execute()\nresult.summary()\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-version","title":"Stable version","text":"<p>The latest stable release of <code>causal-validation</code> can be installed via <code>pip</code>:</p> <pre><code>pip install causal-validation\n</code></pre> <p>We recommend you check your installation version:</p> <pre><code>python -c 'import causal_validation; print(causal_validation.__version__)'\n</code></pre>"},{"location":"installation/#development-version","title":"Development version","text":"<p>Warning</p> <p>This version is possibly unstable and may contain bugs.</p> <p>The latest development version of <code>causal_validation</code> can be installed via running following:</p> <pre><code>git clone git@github.com:amazon-science/causal-validation.git;\ncd causal-validation;\nhatch shell create\n</code></pre> <p>We advise you create virtual environment before installing:</p> <pre><code>conda create -n causal-validation python=3.11.0;\nconda activate causal-validation\n</code></pre> <p>and recommend you check your installation passes the supplied unit tests:</p> <pre><code>hatch run dev:test\n</code></pre>"},{"location":"examples/azcausal/","title":"AZCausal Integration","text":"<pre><code>from azcausal.estimators.panel.sdid import SDID\nimport scipy.stats as st\n\nfrom causal_validation import (\n    Config,\n    simulate,\n)\nfrom causal_validation.effects import StaticEffect\nfrom causal_validation.plotters import plot\nfrom causal_validation.transforms import (\n    Periodic,\n    Trend,\n)\nfrom causal_validation.transforms.parameter import UnitVaryingParameter\n</code></pre> <pre><code>cfg = Config(\n    n_control_units=10,\n    n_pre_intervention_timepoints=60,\n    n_post_intervention_timepoints=30,\n    seed=123,\n)\n\nlinear_trend = Trend(degree=1, coefficient=0.05)\ndata = linear_trend(simulate(cfg))\nax = plot(data)\n</code></pre> <p>will inflate the treated group's observations in the post-intervention window.</p> <pre><code>TRUE_EFFECT = 0.05\neffect = StaticEffect(effect=TRUE_EFFECT)\ninflated_data = effect(data)\nax = plot(inflated_data)\n</code></pre> <pre><code>panel = inflated_data.to_azcausal()\nmodel = SDID()\nresult = model.fit(panel)\nprint(f\"Delta: {TRUE_EFFECT - result.effect.percentage().value / 100}\")\nprint(result.summary(title=\"Synthetic Data Experiment\"))\n</code></pre> <pre>\n<code>Delta: -2.3592239273284576e-16\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n|                          Synthetic Data Experiment                           |\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n|                                    Panel                                     |\n|  Time Periods: 90 (60/30)                                  total (pre/post)  |\n|  Units: 11 (10/1)                                       total (contr/treat)  |\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n|                                     ATT                                      |\n|  Effect: 1.1858                                                              |\n|  Observed: 24.90                                                             |\n|  Counter Factual: 23.72                                                      |\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n|                                  Percentage                                  |\n|  Effect: 5.0000                                                              |\n|  Observed: 105.00                                                            |\n|  Counter Factual: 100.00                                                     |\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n|                                  Cumulative                                  |\n|  Effect: 35.57                                                               |\n|  Observed: 747.03                                                            |\n|  Counter Factual: 711.46                                                     |\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>We see that SDID has done an excellent job of estimating the treatment effect.  However, given the simplicity of the data, this is not surprising. With the functionality within this package though we can easily construct more complex datasets in effort to fully stress-test any new model and identify its limitations.</p> <p>To achieve this, we'll simulate 10 control units, 60 pre-intervention time points, and 30 post-intervention time points according to the following process: </p> \\[ \\begin{align} \\mu_{n, t} &amp; \\sim\\mathcal{N}(20, 0.5^2)\\\\ \\alpha_{n} &amp; \\sim \\mathcal{N}(0, 1^2)\\\\ \\beta_{n} &amp; \\sim \\mathcal{N}(0.05, 0.01^2)\\\\ \\nu_n &amp; \\sim \\mathcal{N}(1, 1^2)\\\\ \\gamma_n &amp; \\sim \\operatorname{Student-t}_{10}(1, 1^2)\\\\ \\mathbf{Y}_{n, t} &amp; = \\mu_{n, t} + \\alpha_{n} + \\beta_{n}t + \\nu_n\\sin\\left(3\\times 2\\pi t + \\gamma\\right) + \\delta_{t, n} \\end{align} \\] <p>where the true treatment effect \\(\\delta_{t, n}\\) is 5% when \\(n=1\\) and \\(t\\geq 60\\) and 0 otherwise. Meanwhile, \\(\\mathbf{Y}\\) is the matrix of observations, long in the number of time points and wide in the number of units.</p> <pre><code>cfg = Config(\n    n_control_units=10,\n    n_pre_intervention_timepoints=60,\n    n_post_intervention_timepoints=30,\n    global_mean=20,\n    global_scale=1,\n    seed=123,\n)\n\nintercept = UnitVaryingParameter(sampling_dist=st.norm(loc=0.0, scale=1))\ncoefficient = UnitVaryingParameter(sampling_dist=st.norm(loc=0.05, scale=0.01))\nlinear_trend = Trend(degree=1, coefficient=coefficient, intercept=intercept)\n\namplitude = UnitVaryingParameter(sampling_dist=st.norm(loc=1.0, scale=2))\nshift = UnitVaryingParameter(sampling_dist=st.t(df=10))\nperiodic = Periodic(amplitude=amplitude, shift=shift, frequency=3)\n\ndata = effect(periodic(linear_trend(simulate(cfg))))\nax = plot(data)\n</code></pre> <p>time we see that the delta between the estaimted and true effect is much larger than before.</p> <pre><code>panel = data.to_azcausal()\nmodel = SDID()\nresult = model.fit(panel)\nprint(f\"Delta: {100*(TRUE_EFFECT - result.effect.percentage().value / 100): .2f}%\")\nprint(result.summary(title=\"Synthetic Data Experiment\"))\n</code></pre> <pre>\n<code>Delta:  1.71%\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n|                          Synthetic Data Experiment                           |\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n|                                    Panel                                     |\n|  Time Periods: 90 (60/30)                                  total (pre/post)  |\n|  Units: 11 (10/1)                                       total (contr/treat)  |\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n|                                     ATT                                      |\n|  Effect: 0.728265                                                            |\n|  Observed: 22.88                                                             |\n|  Counter Factual: 22.15                                                      |\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n|                                  Percentage                                  |\n|  Effect: 3.2874                                                              |\n|  Observed: 103.29                                                            |\n|  Counter Factual: 100.00                                                     |\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n|                                  Cumulative                                  |\n|  Effect: 21.85                                                               |\n|  Observed: 686.44                                                            |\n|  Counter Factual: 664.59                                                     |\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"examples/azcausal/#azcausal-integration","title":"AZCausal Integration","text":"<p>Amazon's AZCausal library provides the functionality to fit synthetic control and difference-in-difference models to your data. Integrating the synthetic data generating process of <code>causal_validation</code> with AZCausal is trivial, as we show in this notebook. To start, we'll simulate a toy dataset.</p>"},{"location":"examples/azcausal/#fitting-a-model","title":"Fitting a model","text":"<p>We now have some very toy data on which we may apply a model. For this demonstration we shall use the Synthetic Difference-in-Differences model implemented in AZCausal; however, the approach shown here will work for any model implemented in AZCausal. To achieve this, we must first coerce the data into a format that is digestible for AZCausal. Through the <code>.to_azcausal()</code> method implemented here, this is straightforward to achieve. Once we have a AZCausal compatible dataset, the modelling is very simple by virtue of the clean design of AZCausal.</p>"},{"location":"examples/basic/","title":"Data Synthesis","text":"<pre><code>from itertools import product\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import (\n    norm,\n    poisson,\n)\n\nfrom causal_validation import (\n    Config,\n    simulate,\n)\nfrom causal_validation.effects import StaticEffect\nfrom causal_validation.plotters import plot\nfrom causal_validation.transforms import (\n    Periodic,\n    Trend,\n)\nfrom causal_validation.transforms.parameter import UnitVaryingParameter\n</code></pre> <p>then invoking the <code>simulate</code> function. Once simulated, we may visualise the data through the <code>plot</code> function.</p> <pre><code>cfg = Config(\n    n_control_units=10,\n    n_pre_intervention_timepoints=60,\n    n_post_intervention_timepoints=30,\n    seed=123,\n)\n\ndata = simulate(cfg)\nax = plot(data)\n</code></pre> <pre><code>means = [10, 50]\nscales = [0.1, 0.5]\n\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(10, 6), tight_layout=True)\nfor (m, s), ax in zip(product(means, scales), axes.ravel(), strict=False):\n    cfg = Config(\n        n_control_units=10,\n        n_pre_intervention_timepoints=60,\n        n_post_intervention_timepoints=30,\n        global_mean=m,\n        global_scale=s,\n    )\n    data = simulate(cfg)\n    _ = plot(data, ax=ax, title=f\"Mean: {m}, Scale: {s}\")\n</code></pre> <pre><code>cfg = Config(\n    n_control_units=10,\n    n_pre_intervention_timepoints=60,\n    n_post_intervention_timepoints=30,\n    seed=42,\n)\n</code></pre> <p>Reusing the same config file across simulations</p> <pre><code>fig, axes = plt.subplots(ncols=2, figsize=(10, 3))\nfor ax in axes:\n    data = simulate(cfg)\n    _ = plot(data, ax=ax)\n</code></pre> <p>Or manually specifying and passing your own pseudorandom number generator key</p> <pre><code>rng = np.random.RandomState(42)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 3))\nfor ax in axes:\n    data = simulate(cfg, key=rng)\n    _ = plot(data, ax=ax)\n</code></pre> <pre><code>effect = StaticEffect(effect=0.02)\ninflated_data = effect(data)\nfig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 3))\nax0 = plot(data, ax=ax0, title=\"Original data\")\nax1 = plot(inflated_data, ax=ax1, title=\"Inflated data\")\n</code></pre> <pre><code>trend_term = Trend(degree=1, coefficient=0.1)\ndata_with_trend = effect(trend_term(data))\nax = plot(data_with_trend)\n</code></pre> <pre><code>trend_term = Trend(degree=2, coefficient=0.0025)\ndata_with_trend = effect(trend_term(data))\nax = plot(data_with_trend)\n</code></pre> <p>We may also include periodic components in our data</p> <pre><code>periodicity = Periodic(amplitude=2, frequency=6)\nperioidic_data = effect(periodicity(trend_term(data)))\nax = plot(perioidic_data)\n</code></pre> <pre><code>sampling_dist = norm(0.0, 1.0)\nintercept = UnitVaryingParameter(sampling_dist=sampling_dist)\ntrend_term = Trend(degree=1, intercept=intercept, coefficient=0.1)\ndata_with_trend = effect(trend_term(data))\nax = plot(data_with_trend)\n</code></pre> <pre><code>sampling_dist = poisson(2)\nfrequency = UnitVaryingParameter(sampling_dist=sampling_dist)\n\np = Periodic(frequency=frequency)\nax = plot(p(data))\n</code></pre>"},{"location":"examples/basic/#data-synthesis","title":"Data Synthesis","text":"<p>In this notebook we'll demonstrate how <code>causal-validation</code> can be used to simulate synthetic datasets. We'll start with very simple data to which a static treatment effect may be applied. From there, we'll build up to complex datasets. Along the way, we'll show how reproducibility can be ensured, plots can be generated, and unit-level parameters may be specified.</p>"},{"location":"examples/basic/#simulating-a-dataset","title":"Simulating a Dataset","text":""},{"location":"examples/basic/#controlling-baseline-behaviour","title":"Controlling baseline behaviour","text":"<p>We observe that we have 10 control units, each of which were sampled from a Gaussian distribution with mean 20 and scale 0.2. Had we wished for our underlying observations to have more or less noise, or to have a different global mean, then we can simply specify that through the config file.</p>"},{"location":"examples/basic/#reproducibility","title":"Reproducibility","text":"<p>In the above four panels, we can see that whilst the mean and scale of the underlying data generating process is varying, the functional form of the data is the same. This is by design to ensure that data sampling is reproducible. To sample a new dataset, you may either change the underlying seed in the config file.</p>"},{"location":"examples/basic/#simulating-an-effect","title":"Simulating an effect","text":"<p>In the data we have seen up until now, the treated unit has been drawn from the same data generating process as the control units. However, it can be helpful to also inflate the treated unit to observe how well our model can recover the the true treatment effect. To do this, we simply compose our dataset with an <code>Effect</code> object. In the below, we shall inflate our data by 2%.</p>"},{"location":"examples/basic/#more-complex-generation-processes","title":"More complex generation processes","text":"<p>The example presented above shows a very simple stationary data generation process. However, we may make our example more complex by including a non-stationary trend to the data.</p>"},{"location":"examples/basic/#unit-level-parameterisation","title":"Unit-level parameterisation","text":""},{"location":"examples/basic/#conclusions","title":"Conclusions","text":"<p>In this notebook we have shown how one can define their model's true underlying data generating process, starting from simple white-noise samples through to more complex example with periodic and temporal components, perhaps containing unit-level variation. In a follow-up notebook, we show how these datasets may be integrated with Amazon's own AZCausal library to compare the effect estimated by a model with the true effect of the underlying data generating process. A link to this notebook is here.</p>"},{"location":"examples/placebo_test/","title":"Placebo Testing","text":"<pre><code>from azcausal.core.error import JackKnife\nfrom azcausal.estimators.panel.did import DID\nfrom azcausal.estimators.panel.sdid import SDID\nfrom scipy.stats import norm\n\nfrom causal_validation import (\n    Config,\n    simulate,\n)\nfrom causal_validation.data import DatasetContainer\nfrom causal_validation.effects import StaticEffect\nfrom causal_validation.models import AZCausalWrapper\nfrom causal_validation.plotters import plot\nfrom causal_validation.transforms import Trend\nfrom causal_validation.validation.placebo import PlaceboTest\nfrom causal_validation.transforms.parameter import UnitVaryingParameter\n</code></pre> <pre>\n<code>/home/runner/.local/share/hatch/env/virtual/causal-validation/CYBYs5D-/docs/lib/python3.10/site-packages/pandera/engines/pandas_engine.py:67: UserWarning: Using typeguard &lt; 3. Generic types like List[TYPE], Dict[TYPE, TYPE] will only validate the first element in the collection.\n  warnings.warn(\n</code>\n</pre> <pre><code>cfg = Config(\n    n_control_units=10,\n    n_pre_intervention_timepoints=60,\n    n_post_intervention_timepoints=30,\n    seed=123,\n)\n\nTRUE_EFFECT = 0.05\neffect = StaticEffect(effect=TRUE_EFFECT)\ndata = effect(simulate(cfg))\nax = plot(data)\n</code></pre> <pre><code>model = AZCausalWrapper(model=SDID(), error_estimator=JackKnife())\n</code></pre> <pre><code>result = PlaceboTest(model, datasets=data).execute()\nresult.summary()\n</code></pre> <pre></pre> <pre>                                                                              \n| Model | Dataset   | Effect | Standard Deviation | Standard Error | p-value |\n|-------|-----------|--------|--------------------|----------------|---------|\n| SDID  | Dataset 0 | 0.0851 | 0.4079             | 0.129          | 0.5472  |\n\n</pre> <pre><code>data.name = \"Simple\"\ndid_model = AZCausalWrapper(model=DID())\nPlaceboTest([model, did_model], data).execute().summary()\n</code></pre> <pre></pre> <pre>                                                                            \n| Model | Dataset | Effect | Standard Deviation | Standard Error | p-value |\n|-------|---------|--------|--------------------|----------------|---------|\n| SDID  | Simple  | 0.0851 | 0.4079             | 0.129          | 0.5472  |\n| DID   | Simple  | 0.0002 | 0.2818             | 0.0891         | 0.9982  |\n\n</pre> <p>Sometimes, we may wish to compare multiple models across multiple datasets. Fortunately, this is as simple as providing the datasets as a list to <code>PlaceboTest</code>, much like the models we supplied in the previous cell. From here, <code>causal-validation</code> will go and conduct a placebo test for each pair of model and dataset. To see this, let us now synthesis a more complex dataset where the intercept and slope of each control unit vary.</p> <pre><code>cfg2 = Config(\n    n_control_units=10,\n    n_pre_intervention_timepoints=60,\n    n_post_intervention_timepoints=30,\n    seed=42,\n)\n\n\neffect = StaticEffect(effect=TRUE_EFFECT)\nintercept = UnitVaryingParameter(sampling_dist=norm(0.0, 1.0))\nslope = UnitVaryingParameter(sampling_dist=norm(0.2, 0.05))\ntrend = Trend(degree=1, coefficient=slope, intercept=intercept)\ncomplex_data = effect(trend(simulate(cfg2)))\n</code></pre> <p>By default, the data will be named according to the index it was supplied, as seen in previous cells. However, it can be named by wrapping the datasets up in a <code>DatasetContainer</code> object and supplying the datasets' names as a list of strings.</p> <pre><code>datasets = DatasetContainer([data, complex_data], names=[\"Simple\", \"Complex\"])\nPlaceboTest([model, did_model], datasets).execute().summary()\n</code></pre> <pre></pre> <pre>                                                                             \n| Model | Dataset | Effect  | Standard Deviation | Standard Error | p-value |\n|-------|---------|---------|--------------------|----------------|---------|\n| SDID  | Simple  | 0.0851  | 0.4079             | 0.129          | 0.5472  |\n| DID   | Simple  | 0.0002  | 0.2818             | 0.0891         | 0.9982  |\n| SDID  | Complex | -0.1391 | 0.8806             | 0.2785         | 0.6469  |\n| DID   | Complex | -0.7316 | 9.1745             | 2.9012         | 0.8163  |\n\n</pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/placebo_test/#placebo-testing","title":"Placebo Testing","text":"<p>A placebo test is an approach to assess the validity of a causal model by checking if the effect can truly be attributed to the treatment, or to other spurious factors. A placebo test is conducted by iterating through the set of control units and at each iteration, replacing the treated unit by one of the control units and measuring the effect. If the model detects a significant effect, then it suggests potential bias or omitted variables in the analysis, indicating that the causal inference is flawed.</p> <p>A successful placebo test will show no statistically significant results and we may then conclude that the estimated effect can be attributed to the treatment and not driven by confounding factors. Conversely, a failed placebo test, which shows significant results, suggests that the identified treatment effect may not be reliable. Placebo testing is thus a critical step to ensure the robustness of findings in RCTs. In this notebook, we demonstrate how a placebo test can be conducted in <code>causal-validation</code>.</p>"},{"location":"examples/placebo_test/#data-simulation","title":"Data simulation","text":"<p>To demonstrate a placebo test, we must first simulate some data. For the purposes of illustration, we'll simulate a very simple dataset containing 10 control units where each unit has 60 pre-intervention observations, and 30 post-intervention observations.</p>"},{"location":"examples/placebo_test/#model","title":"Model","text":"<p>We'll now define our model. To do this, we'll use the synthetic difference-in-differences implementation of AZCausal. This implementation, along with any other model from AZCausal, can be neatly wrapped up in our <code>AZCausalWrapper</code> to make fitting and effect estimation simpler.</p>"},{"location":"examples/placebo_test/#placebo-test-results","title":"Placebo Test Results","text":"<p>Now that we have a dataset and model defined, we may conduct our placebo test. With 10 control units, the test will estimate 10 individual effects; 1 per control unit when it is mocked as the treated group. With those 10 effects, the routine will then produce the mean estimated effect, along with the standard deviation across the estimated effect, the effect's standard error, and the p-value that corresponds to the null-hypothesis test that the effect is 0.</p> <p>In the below, we see that expected estimated effect is small at just 0.08. Accordingly, the p-value attains a value of 0.5, indicating that we have insufficient evidence to reject the null hypothesis and we, therefore, have no evidence to suggest that there is bias within this particular setup.</p>"},{"location":"examples/placebo_test/#model-comparison","title":"Model Comparison","text":"<p>We can also use the results of a placebo test to compare two or more models. Using <code>causal-validation</code>, this is as simple as supplying a series of models to the placebo test and comparing their outputs. To demonstrate this, we will compare the previously used synthetic difference-in-differences model with regular difference-in-differences. In the previous placebo test you'll notice that the dataset in use was named <code>\"Dataset 0\"</code>. We can set the <code>name</code> property of our <code>Dataset</code> to more cleanly log the name of the data under consideration.</p>"}]}